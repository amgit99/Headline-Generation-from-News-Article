{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\nimport pickle\nimport random\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nimport math\nfrom torch import optim\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.nn import TransformerDecoder, TransformerDecoderLayer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# %% [code]\nclass PositionalEncoding(torch.nn.Module):\n\n    def __init__(self, d_model, dropout = 0.1, max_len = 62):\n        super().__init__()\n        self.dropout = torch.nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n\n# %% [code]\ndef generate_padding_mask(src, pad_idx = {0,1,2}):\n        x = (src == 0)\n        x = (x == 1)\n        return (x == 2).transpose(1, 0)\n    \ndef generate_attn_mask(tgt_sz, src_sz):\n    mask = (torch.triu(torch.ones(src_sz, tgt_sz)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n# %% [code]\nclass OptimasPrim(torch.nn.Module):\n    def __init__(self, embedding_dim, vocab_size, embedding_weights_matrix):\n        super(OptimasPrim, self).__init__()\n        self.embedding = torch.nn.Embedding.from_pretrained(embedding_weights_matrix, freeze = False)\n        self.input_pos_encoding = PositionalEncoding(embedding_dim, max_len = 60)\n        self.target_pos_encoding = PositionalEncoding(embedding_dim, max_len = 10)\n        self.transformer = torch.nn.Transformer(d_model=embedding_dim, nhead = 10, dropout = 1)\n        self.out = torch.nn.Linear(embedding_dim, vocab_size)\n            \n    def forward(self, inputs, targets):\n        embedded_inputs = self.embedding(inputs)\n        embedded_targets = self.embedding(targets)\n        embedded_inputs = self.input_pos_encoding(embedded_inputs)\n        embedded_targets = self.target_pos_encoding(embedded_targets)\n        t_mask = generate_attn_mask(10, 10).to(device)\n        m_mask = generate_attn_mask(10, 60).to(device)\n        skpm = generate_padding_mask(inputs).to(device)\n        tkpm = generate_padding_mask(targets).to(device)\n        output = self.transformer(embedded_inputs, embedded_targets, tgt_mask = t_mask,\n                                  src_key_padding_mask = skpm, tgt_key_padding_mask = tkpm)\n#         output = self.transformer(embedded_inputs, embedded_targets, tgt_mask = t_mask)\n#         output = self.transformer(embedded_inputs, embedded_targets)\n        output = self.out(output.view(-1, 200))\n        return output\n        \n\n# %% [code]\n\n\n# %% [code]\ndef trainIters(transfomer, data_loader, epochs, learning_rate=0.0001):\n    \n    transformer_optimizer = optim.Adam(transfomer.parameters(), lr=learning_rate)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    for epo in range(epochs):\n        for batch_idx, (input_tensor, target_tensor) in tqdm(enumerate(data_loader)):\n            transformer_optimizer.zero_grad()\n            output = transformer(input_tensor.permute(1,0), target_tensor.permute(1,0))\n            loss = criterion(output, target_tensor.view(-1))\n            _, topi = torch.topk(output, 2)\n            lol = {}\n            mul = 1\n            for val in topi.view(-1):\n                if val.item() not in lol: lol[val.item()]=1\n                else: lol[val.item()]+=1\n                mul = max(mul, lol[val.item()])\n            loss*=(0.25*mul**2)\n            loss.backward()\n            transformer_optimizer.step()\n            \n            if (batch_idx+1) % 300 == 0:\n                print(f'Epoch [{epo+1}/{epochs}], Batch [{batch_idx+1}/{len(data_loader)}], Loss: {loss.item():.4f}')\n                evaluate(random.randint(1,10000))\n                print(mul)\n            \n        print(f'\\n=================== EPOCH [{epo+1}/{epochs}] FINISHED ===================\\n') \n        torch.save(transfomer.state_dict(), T_PATH)\n        print(\"=================== MODELS SAVED =====================\")\n        transfomer.load_state_dict(torch.load(T_PATH))\n        print(\"=================== MODELS LOADED ====================\\n\")\n\n# %% [code]\n# Load dictionaries pkl file\nwith open('/kaggle/input/d/amitmarathe/preprocessed-v2/word2index.pickle', 'rb') as fp:\n    word2index = pickle.load(fp)\n    \nwith open('/kaggle/input/d/amitmarathe/preprocessed-v2/index2word.pickle', 'rb') as fp:\n    index2word = pickle.load(fp)\n    \n# Load Dataset\nX = torch.load('/kaggle/input/d/amitmarathe/preprocessed-v2/articles.pt').to(device)\ny = torch.load('/kaggle/input/d/amitmarathe/preprocessed-v2/headlines.pt').to(device)\nembedding_weights_matrix = torch.load('/kaggle/input/d/amitmarathe/preprocessed-v2/embeddings.pt').to(device)\n\n# %% [code]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\nBATCH_SIZE = 1\n\ntrain_data = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True)\n\ntest_data = TensorDataset(X_test, y_test)\ntest_loader = DataLoader(test_data, batch_size = BATCH_SIZE, shuffle=False)\n\n# %% [code]\nEMBEDDING_SIZE = 200\nATTENTION_DIM = 128\nHIDDEN_SIZE = 256\nVOCAB_SIZE = len(word2index)\nT_PATH=\"transformer.pt\"\n\n# %% [code]\ntransformer = OptimasPrim(EMBEDDING_SIZE, VOCAB_SIZE, embedding_weights_matrix).to(device)\ntrainIters(transformer, train_loader, 1)\n\n# %% [code]\ndef evaluate(index):\n    art, head = train_data[index]\n    print(\"\\n ARTICLE IS :: \\n\")\n    for word in art:\n        print(index2word[word.item()], end = ' ')\n    print(\"\\n\\n HEADLINE IS :: \\n\")\n    for word in head:\n        print(index2word[word.item()], end = ' ')\n            \n    print(\"\\n\\n PREDICTED HEADLINE IS :: \\n\")\n    input_tensor = art.reshape(-1,1)\n    target_tensor = head.reshape(-1,1)\n    out = transformer(input_tensor, target_tensor)\n    \n    for di in range(10):\n        _, topi = torch.topk(out[di], 2)\n        print(index2word[topi[1].item()], end = ' ')\n    print()\n\n# %% [code]\ntransformer = OptimasPrim(EMBEDDING_SIZE, VOCAB_SIZE, embedding_weights_matrix).to(device)\ntransformer.load_state_dict(torch.load('/kaggle/input/transformer-model/transformer.pt'))\n\n# %% [code]\nevaluate(7)\n\n# %% [code]\n","metadata":{"_uuid":"e3497e54-cdd5-4e7a-bc11-ed173b14e27b","_cell_guid":"c011156b-6db0-406e-b478-4f057095c0ad","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}