{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\nimport pickle\nimport random\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nfrom torch import optim\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# %% [code]\nclass Encoder(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, embedding_matrix):\n        super(Encoder, self).__init__()\n        self.embedding = torch.nn.Embedding.from_pretrained(embedding_matrix, freeze = False)\n        self.enc_rnn = torch.nn.GRU(input_size, hidden_size, batch_first = False, num_layers = 2, dropout = 0.75)\n        \n    def forward(self, input):\n        seq, bat = input.size()\n        enc_input = self.embedding(input)\n        output, hidden = self.enc_rnn(enc_input)\n        return output, hidden\n\n# %% [code]\nclass AttnDecoderRNN(torch.nn.Module):\n    def __init__(self, input_size, hidden_size, attention_dim, input_seq_len, output_seq_len, embedding_matrix, vocab_size):\n        super(AttnDecoderRNN, self).__init__()\n        self.input_seq = input_seq_len\n        self.output_seq = output_seq_len\n\n        self.embedding = torch.nn.Embedding.from_pretrained(embedding_matrix, freeze = False)\n        self.time_step_attn_layer = torch.nn.Linear(2*hidden_size, attention_dim)\n        self.input_seq_attn_layer = torch.nn.Linear(hidden_size, attention_dim)\n        self.alpha_attn = torch.nn.Linear(2*attention_dim, 1)\n        self.dec_rnn = torch.nn.GRU(input_size, hidden_size, batch_first=False, num_layers = 2, dropout = 0.75)\n        self.out = torch.nn.Linear(hidden_size, vocab_size)\n        self.nl = torch.nn.ELU()\n\n    def forward(self, input, hidden, encoder_outputs):\n        embedded = self.embedding(input)\n        _, bat, embed = embedded.size()\n        _, _, hid = hidden.size()\n         \n        attn_hidden = hidden.permute(1,0,2).reshape(-1, 2*hid)\n        time_step_attn = self.time_step_attn_layer(attn_hidden)\n        time_step_attn = self.nl(time_step_attn)\n        alphas = torch.zeros(self.input_seq, bat, device=device)\n\n        for alpha in range(self.input_seq):\n\n            input_attn = self.input_seq_attn_layer(encoder_outputs[alpha])\n            input_attn = self.nl(input_attn)\n            input_timestep_attn = torch.cat((input_attn, time_step_attn), 1)\n            alphas[alpha] = (self.alpha_attn(input_timestep_attn).view(-1))\n\n        alphas = alphas.permute(1,0)\n        alphas = F.log_softmax(alphas, dim = 1).view(bat, -1, 1) \n        attn_applied = torch.bmm(encoder_outputs.permute(1,2,0), alphas).permute(2,0,1)\n        \n        dec_input = torch.cat((embedded, attn_applied), 2)\n        output, hidden = self.dec_rnn(dec_input, hidden)\n\n        output = F.log_softmax(self.out(output.view(bat,-1)), dim=1)\n        return output, hidden, alphas\n\n# %% [code]\nteacher_forcing_ratio = 0.25\nSOS_token = 0\nEOS_token = 1\n\ndef train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    loss = 0\n\n    encoder_outputs, encoder_hidden = encoder(input_tensor)\n    decoder_input = torch.zeros((1,input_tensor.size()[1]), device=device, dtype= torch.long)\n    decoder_hidden = encoder_hidden\n\n    use_teacher_forcing = 1\n    \n    for di in range(9):\n        decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n        loss += criterion(decoder_output, target_tensor[di])\n        if use_teacher_forcing:\n            decoder_input = target_tensor[di].view(1,-1)\n        else:\n            _, topi = decoder_output.topk(1)\n            decoder_input = topi.detach().view(1,-1)\n        \n        _, topi = torch.topk(decoder_output, 2)\n        lol = {}\n        mul = 1\n        for val in topi.view(-1):\n            if val.item() not in lol: lol[val.item()]=1\n            else: lol[val.item()]+=1\n            mul = max(mul, lol[val.item()])\n            \n    loss *= 0.1*(mul**2)\n \n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item()\n\n# %% [code]\ndef trainIters(encoder, decoder, data_loader, epochs, learning_rate=0.001):\n    \n    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    for epo in range(epochs):\n        \n        for batch_idx, (input_tensor, target_tensor) in enumerate(data_loader):\n            input_tensor = input_tensor.permute(1,0)\n            target_tensor = target_tensor.permute(1,0)\n            \n            loss = train(input_tensor.to(device), target_tensor.to(device), encoder, decoder, \n                         encoder_optimizer, decoder_optimizer, criterion)\n            \n            if (batch_idx+1) % 50 == 0:\n                print(f'Epoch [{epo+1}/{epochs}], Batch [{batch_idx+1}/{len(data_loader)}], Loss: {loss:.4f}')\n            \n        print(f'\\n=================== EPOCH [{epo+1}/{epochs}] FINISHED ===================\\n') \n        torch.save(encoder.state_dict(), E_PATH)\n        torch.save(attn_decoder.state_dict(), D_PATH)\n        print(\"=================== MODELS SAVED =====================\")\n        encoder.load_state_dict(torch.load(E_PATH))\n        attn_decoder.load_state_dict(torch.load(D_PATH))\n        print(\"=================== MODELS LOADED ====================\\n\")\n\n# %% [code]\n# Load dictionaries pkl file\nwith open('/kaggle/input/preprocessed-v2/word2index.pickle', 'rb') as fp:\n    word2index = pickle.load(fp)\n    \nwith open('/kaggle/input/preprocessed-v2/index2word.pickle', 'rb') as fp:\n    index2word = pickle.load(fp)\n    \n# Load Dataset\nX = torch.load('/kaggle/input/preprocessed-v2/articles.pt')\ny = torch.load('/kaggle/input/preprocessed-v2/headlines.pt')\nembedding_weights_matrix = torch.load('/kaggle/input/preprocessed-v2/embeddings.pt')\n\n# %% [code]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\nBATCH_SIZE = 64\n\ntrain_data = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True)\n\ntest_data = TensorDataset(X_test, y_test)\ntest_loader = DataLoader(test_data, batch_size = BATCH_SIZE, shuffle=False)\n\n# %% [code]\nEMBEDDING_SIZE = 200\nATTENTION_DIM = 128\nHIDDEN_SIZE = 256\nVOCAB_SIZE = len(word2index)\nE_PATH=\"encoder.pt\"\nD_PATH=\"attn_decoder.pt\"\n\n# %% [code]\nencoder = Encoder(EMBEDDING_SIZE, HIDDEN_SIZE, embedding_weights_matrix).to(device)\nattn_decoder = AttnDecoderRNN(EMBEDDING_SIZE + HIDDEN_SIZE, HIDDEN_SIZE, ATTENTION_DIM, 60, 10, embedding_weights_matrix, VOCAB_SIZE).to(device)\ntrainIters(encoder, attn_decoder, train_loader, 20)\n\n# %% [code]\nEMBEDDING_SIZE = 200\nATTENTION_DIM = 128\nHIDDEN_SIZE = 256\nVOCAB_SIZE = len(word2index)\nE_PATH=\"encoder.pt\"\nD_PATH=\"attn_decoder.pt\"\nencoder = Encoder(EMBEDDING_SIZE, HIDDEN_SIZE, embedding_weights_matrix).to(device)\nattn_decoder = AttnDecoderRNN(EMBEDDING_SIZE + HIDDEN_SIZE, HIDDEN_SIZE, ATTENTION_DIM, 60, 10, embedding_weights_matrix, VOCAB_SIZE).to(device)\nencoder.load_state_dict(torch.load('/kaggle/working/encoder.pt'))\nattn_decoder.load_state_dict(torch.load('/kaggle/working/attn_decoder.pt'))\n\n# %% [code]\nevaluate(1030)\n\n# %% [code]\ndef evaluate(index):\n    art, head = test_data[index]\n    print(\"\\n ARTICLE IS :: \\n\")\n    for word in art:\n        print(index2word[word.item()], end = ' ')\n        if(index2word[word.item()]=='<EOS>'): break\n    print(\"\\n\\n HEADLINE IS :: \\n\")\n    for word in head:\n        print(index2word[word.item()], end = ' ')\n        if(index2word[word.item()]=='<EOS>'): break \n            \n    print(\"\\n\\n PREDICTED HEADLINE IS :: \\n\")\n    input_tensor = art.reshape(-1,1).to(device)\n    \n    encoder_outputs, encoder_hidden = encoder(input_tensor)\n    decoder_input = torch.zeros((1,input_tensor.size()[1]), device=device, dtype= torch.long)\n    decoder_hidden = encoder_hidden\n    attention = torch.zeros(16, 62) \n    for di in range(16):\n        decoder_output, decoder_hidden = attn_decoder(decoder_input, decoder_hidden, encoder_outputs)\n        _, topi = decoder_output.topk(1)\n        decoder_input = topi.detach().view(1,-1)\n        print(index2word[topi[0].item()], end = ' ')\n        if(index2word[topi[0].item()]=='<EOS>'): break\n    \n        \n\n# %% [code]\ndef eval_test():\n    all_preds = []\n    all_targets = []\n    for index in tqdm(range(1000)):\n        art, head = test_data[index]\n        input_tensor = art.reshape(-1,1)\n        target_tensor = head.reshape(-1,1)\n\n        encoder_outputs, encoder_hidden = encoder(input_tensor.to(device))\n        decoder_input = torch.zeros((1,input_tensor.size()[1]), device=device, dtype= torch.long)\n        decoder_hidden = encoder_hidden\n        target = [index2word[el.item()] for el in target_tensor[0:10]]\n        predicted =  []\n        for di in range(10):\n            decoder_output, decoder_hidden, _ = attn_decoder(decoder_input, decoder_hidden, encoder_outputs)\n            _, topi = decoder_output.topk(1)\n            decoder_input = topi.detach().view(1,-1)\n            predicted.append(index2word[topi[0].item()])\n        all_preds.append(' '.join(predicted))\n        all_targets.append(' '.join(target))\n    return all_preds, all_targets\n\n# %% [code]\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\n!pip install rouge\nfrom rouge import Rouge\ndef cosine_sim(generated_headlines, reference_headlines):\n    sims = []\n    for gen, ref in zip(generated_headlines, reference_headlines):\n        vectorizer = TfidfVectorizer()\n        tfidf_matrix = vectorizer.fit_transform([gen, ref])\n        sims.append(cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0])\n    return max(sims)\ndef bleu_score(generated_headlines, reference_headlines):\n    generated_tokens = [nltk.word_tokenize(headline) for headline in generated_headlines]\n    reference_tokens = [nltk.word_tokenize(headline) for headline in reference_headlines]\n    return nltk.translate.bleu_score.corpus_bleu(generated_tokens, reference_tokens)\ndef rogue_score(generated_headlines, reference_headlines):\n    rouge = Rouge()\n    return rouge.get_scores(generated_headlines, reference_headlines, avg=True)\n\n# %% [code]\nprint('bleu_score', bleu_score(generated_headlines, reference_headlines))\n\n# %% [code]\n# generated_headlines, reference_headlines = eval_test()\nprint('cosine_sim', cosine_sim(generated_headlines, reference_headlines))\nprint('bleu_score', bleu_score(generated_headlines, reference_headlines))\nprint('rogue_score', rogue_score(generated_headlines, reference_headlines))\n\n# %% [code]\n","metadata":{"_uuid":"4e5ba504-ec29-4794-ad9b-e15416dcfa23","_cell_guid":"51ced860-82f8-47c0-bdf4-43acb6ae4464","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}